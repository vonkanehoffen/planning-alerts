# This file was generated based on ".graphqlconfig". Do not edit manually.

schema {
  query: query_root
  mutation: mutation_root
  subscription: subscription_root
}

"mutation root"
type mutation_root {
  "delete data from the table: \"pa_scrape\""
  delete_pa_scrape(
    #filter the rows which have to be deleted
    where: pa_scrape_bool_exp!
  ): pa_scrape_mutation_response
  "delete data from the table: \"pa_scrape_validated_depreciated\""
  delete_pa_scrape_validated_depreciated(
    #filter the rows which have to be deleted
    where: pa_scrape_validated_depreciated_bool_exp!
  ): pa_scrape_validated_depreciated_mutation_response
  "delete data from the table: \"pa_status\""
  delete_pa_status(
    #filter the rows which have to be deleted
    where: pa_status_bool_exp!
  ): pa_status_mutation_response
  "delete data from the table: \"planning_app\""
  delete_planning_app(
    #filter the rows which have to be deleted
    where: planning_app_bool_exp!
  ): planning_app_mutation_response
  "delete data from the table: \"scrape_log\""
  delete_scrape_log(
    #filter the rows which have to be deleted
    where: scrape_log_bool_exp!
  ): scrape_log_mutation_response
  "delete data from the table: \"users\""
  delete_users(
    #filter the rows which have to be deleted
    where: users_bool_exp!
  ): users_mutation_response
  "insert data into the table: \"pa_scrape\""
  insert_pa_scrape(
    #the rows to be inserted
    objects: [pa_scrape_insert_input!]!,
    #on conflict condition
    on_conflict: pa_scrape_on_conflict
  ): pa_scrape_mutation_response
  "insert data into the table: \"pa_scrape_validated_depreciated\""
  insert_pa_scrape_validated_depreciated(
    #the rows to be inserted
    objects: [pa_scrape_validated_depreciated_insert_input!]!,
    #on conflict condition
    on_conflict: pa_scrape_validated_depreciated_on_conflict
  ): pa_scrape_validated_depreciated_mutation_response
  "insert data into the table: \"pa_status\""
  insert_pa_status(
    #the rows to be inserted
    objects: [pa_status_insert_input!]!,
    #on conflict condition
    on_conflict: pa_status_on_conflict
  ): pa_status_mutation_response
  "insert data into the table: \"planning_app\""
  insert_planning_app(
    #the rows to be inserted
    objects: [planning_app_insert_input!]!,
    #on conflict condition
    on_conflict: planning_app_on_conflict
  ): planning_app_mutation_response
  "insert data into the table: \"scrape_log\""
  insert_scrape_log(
    #the rows to be inserted
    objects: [scrape_log_insert_input!]!,
    #on conflict condition
    on_conflict: scrape_log_on_conflict
  ): scrape_log_mutation_response
  "insert data into the table: \"users\""
  insert_users(
    #the rows to be inserted
    objects: [users_insert_input!]!,
    #on conflict condition
    on_conflict: users_on_conflict
  ): users_mutation_response
  "update data of the table: \"pa_scrape\""
  update_pa_scrape(
    #append existing jsonb value of filtered columns with new jsonb value
    _append: pa_scrape_append_input,
    #delete the field or element with specified path (for JSON arrays, negative integers count from the end)
    _delete_at_path: pa_scrape_delete_at_path_input,
    #delete the array element with specified index (negative integers count from the end). throws an error if top level container is not an array
    _delete_elem: pa_scrape_delete_elem_input,
    #delete key/value pair or string element. key/value pairs are matched based on their key value
    _delete_key: pa_scrape_delete_key_input,
    #increments the integer columns with given value of the filtered values
    _inc: pa_scrape_inc_input,
    #prepend existing jsonb value of filtered columns with new jsonb value
    _prepend: pa_scrape_prepend_input,
    #sets the columns of the filtered rows to the given values
    _set: pa_scrape_set_input,
    #filter the rows which have to be updated
    where: pa_scrape_bool_exp!
  ): pa_scrape_mutation_response
  "update data of the table: \"pa_scrape_validated_depreciated\""
  update_pa_scrape_validated_depreciated(
    #append existing jsonb value of filtered columns with new jsonb value
    _append: pa_scrape_validated_depreciated_append_input,
    #delete the field or element with specified path (for JSON arrays, negative integers count from the end)
    _delete_at_path: pa_scrape_validated_depreciated_delete_at_path_input,
    #delete the array element with specified index (negative integers count from the end). throws an error if top level container is not an array
    _delete_elem: pa_scrape_validated_depreciated_delete_elem_input,
    #delete key/value pair or string element. key/value pairs are matched based on their key value
    _delete_key: pa_scrape_validated_depreciated_delete_key_input,
    #increments the integer columns with given value of the filtered values
    _inc: pa_scrape_validated_depreciated_inc_input,
    #prepend existing jsonb value of filtered columns with new jsonb value
    _prepend: pa_scrape_validated_depreciated_prepend_input,
    #sets the columns of the filtered rows to the given values
    _set: pa_scrape_validated_depreciated_set_input,
    #filter the rows which have to be updated
    where: pa_scrape_validated_depreciated_bool_exp!
  ): pa_scrape_validated_depreciated_mutation_response
  "update data of the table: \"pa_status\""
  update_pa_status(
    #sets the columns of the filtered rows to the given values
    _set: pa_status_set_input,
    #filter the rows which have to be updated
    where: pa_status_bool_exp!
  ): pa_status_mutation_response
  "update data of the table: \"planning_app\""
  update_planning_app(
    #append existing jsonb value of filtered columns with new jsonb value
    _append: planning_app_append_input,
    #delete the field or element with specified path (for JSON arrays, negative integers count from the end)
    _delete_at_path: planning_app_delete_at_path_input,
    #delete the array element with specified index (negative integers count from the end). throws an error if top level container is not an array
    _delete_elem: planning_app_delete_elem_input,
    #delete key/value pair or string element. key/value pairs are matched based on their key value
    _delete_key: planning_app_delete_key_input,
    #prepend existing jsonb value of filtered columns with new jsonb value
    _prepend: planning_app_prepend_input,
    #sets the columns of the filtered rows to the given values
    _set: planning_app_set_input,
    #filter the rows which have to be updated
    where: planning_app_bool_exp!
  ): planning_app_mutation_response
  "update data of the table: \"scrape_log\""
  update_scrape_log(
    #append existing jsonb value of filtered columns with new jsonb value
    _append: scrape_log_append_input,
    #delete the field or element with specified path (for JSON arrays, negative integers count from the end)
    _delete_at_path: scrape_log_delete_at_path_input,
    #delete the array element with specified index (negative integers count from the end). throws an error if top level container is not an array
    _delete_elem: scrape_log_delete_elem_input,
    #delete key/value pair or string element. key/value pairs are matched based on their key value
    _delete_key: scrape_log_delete_key_input,
    #increments the integer columns with given value of the filtered values
    _inc: scrape_log_inc_input,
    #prepend existing jsonb value of filtered columns with new jsonb value
    _prepend: scrape_log_prepend_input,
    #sets the columns of the filtered rows to the given values
    _set: scrape_log_set_input,
    #filter the rows which have to be updated
    where: scrape_log_bool_exp!
  ): scrape_log_mutation_response
  "update data of the table: \"users\""
  update_users(
    #sets the columns of the filtered rows to the given values
    _set: users_set_input,
    #filter the rows which have to be updated
    where: users_bool_exp!
  ): users_mutation_response
}

"columns and relationships of \"pa_scrape\""
type pa_scrape {
  contacts(
    #JSON select path
    path: String
  ): jsonb
  further_information(
    #JSON select path
    path: String
  ): jsonb
  id: Int!
  important_dates(
    #JSON select path
    path: String
  ): jsonb
  list_type: String!
  "An object relationship"
  pa_status: pa_status!
  reference: String!
  scraped_at: timestamptz!
  scraper: String!
  summary(
    #JSON select path
    path: String
  ): jsonb
  url: String!
}

"aggregated selection of \"pa_scrape\""
type pa_scrape_aggregate {
  aggregate: pa_scrape_aggregate_fields
  nodes: [pa_scrape!]!
}

"aggregate fields of \"pa_scrape\""
type pa_scrape_aggregate_fields {
  avg: pa_scrape_avg_fields
  count(columns: [pa_scrape_select_column!], distinct: Boolean): Int
  max: pa_scrape_max_fields
  min: pa_scrape_min_fields
  stddev: pa_scrape_stddev_fields
  stddev_pop: pa_scrape_stddev_pop_fields
  stddev_samp: pa_scrape_stddev_samp_fields
  sum: pa_scrape_sum_fields
  var_pop: pa_scrape_var_pop_fields
  var_samp: pa_scrape_var_samp_fields
  variance: pa_scrape_variance_fields
}

"aggregate avg on columns"
type pa_scrape_avg_fields {
  id: Float
}

"aggregate max on columns"
type pa_scrape_max_fields {
  id: Int
  list_type: String
  reference: String
  scraped_at: timestamptz
  scraper: String
  url: String
}

"aggregate min on columns"
type pa_scrape_min_fields {
  id: Int
  list_type: String
  reference: String
  scraped_at: timestamptz
  scraper: String
  url: String
}

"response of any mutation on the table \"pa_scrape\""
type pa_scrape_mutation_response {
  "number of affected rows by the mutation"
  affected_rows: Int!
  "data of the affected rows by the mutation"
  returning: [pa_scrape!]!
}

"aggregate stddev on columns"
type pa_scrape_stddev_fields {
  id: Float
}

"aggregate stddev_pop on columns"
type pa_scrape_stddev_pop_fields {
  id: Float
}

"aggregate stddev_samp on columns"
type pa_scrape_stddev_samp_fields {
  id: Float
}

"aggregate sum on columns"
type pa_scrape_sum_fields {
  id: Int
}

"columns and relationships of \"pa_scrape_validated_depreciated\""
type pa_scrape_validated_depreciated {
  address: String!
  appeal_decision: String
  appeal_status: String
  application_received: timestamptz
  application_validated: timestamptz
  contacts(
    #JSON select path
    path: String
  ): jsonb!
  decision: String
  decision_issued_date: timestamptz
  further_information(
    #JSON select path
    path: String
  ): jsonb!
  id: Int!
  important_dates(
    #JSON select path
    path: String
  ): jsonb!
  "An object relationship"
  pa_status: pa_status!
  proposal: String!
  reference: String!
  scraped_at: timestamptz!
  scraper: String!
  status: String
  url: String!
}

"aggregated selection of \"pa_scrape_validated_depreciated\""
type pa_scrape_validated_depreciated_aggregate {
  aggregate: pa_scrape_validated_depreciated_aggregate_fields
  nodes: [pa_scrape_validated_depreciated!]!
}

"aggregate fields of \"pa_scrape_validated_depreciated\""
type pa_scrape_validated_depreciated_aggregate_fields {
  avg: pa_scrape_validated_depreciated_avg_fields
  count(columns: [pa_scrape_validated_depreciated_select_column!], distinct: Boolean): Int
  max: pa_scrape_validated_depreciated_max_fields
  min: pa_scrape_validated_depreciated_min_fields
  stddev: pa_scrape_validated_depreciated_stddev_fields
  stddev_pop: pa_scrape_validated_depreciated_stddev_pop_fields
  stddev_samp: pa_scrape_validated_depreciated_stddev_samp_fields
  sum: pa_scrape_validated_depreciated_sum_fields
  var_pop: pa_scrape_validated_depreciated_var_pop_fields
  var_samp: pa_scrape_validated_depreciated_var_samp_fields
  variance: pa_scrape_validated_depreciated_variance_fields
}

"aggregate avg on columns"
type pa_scrape_validated_depreciated_avg_fields {
  id: Float
}

"aggregate max on columns"
type pa_scrape_validated_depreciated_max_fields {
  address: String
  appeal_decision: String
  appeal_status: String
  application_received: timestamptz
  application_validated: timestamptz
  decision: String
  decision_issued_date: timestamptz
  id: Int
  proposal: String
  reference: String
  scraped_at: timestamptz
  scraper: String
  status: String
  url: String
}

"aggregate min on columns"
type pa_scrape_validated_depreciated_min_fields {
  address: String
  appeal_decision: String
  appeal_status: String
  application_received: timestamptz
  application_validated: timestamptz
  decision: String
  decision_issued_date: timestamptz
  id: Int
  proposal: String
  reference: String
  scraped_at: timestamptz
  scraper: String
  status: String
  url: String
}

"response of any mutation on the table \"pa_scrape_validated_depreciated\""
type pa_scrape_validated_depreciated_mutation_response {
  "number of affected rows by the mutation"
  affected_rows: Int!
  "data of the affected rows by the mutation"
  returning: [pa_scrape_validated_depreciated!]!
}

"aggregate stddev on columns"
type pa_scrape_validated_depreciated_stddev_fields {
  id: Float
}

"aggregate stddev_pop on columns"
type pa_scrape_validated_depreciated_stddev_pop_fields {
  id: Float
}

"aggregate stddev_samp on columns"
type pa_scrape_validated_depreciated_stddev_samp_fields {
  id: Float
}

"aggregate sum on columns"
type pa_scrape_validated_depreciated_sum_fields {
  id: Int
}

"aggregate var_pop on columns"
type pa_scrape_validated_depreciated_var_pop_fields {
  id: Float
}

"aggregate var_samp on columns"
type pa_scrape_validated_depreciated_var_samp_fields {
  id: Float
}

"aggregate variance on columns"
type pa_scrape_validated_depreciated_variance_fields {
  id: Float
}

"aggregate var_pop on columns"
type pa_scrape_var_pop_fields {
  id: Float
}

"aggregate var_samp on columns"
type pa_scrape_var_samp_fields {
  id: Float
}

"aggregate variance on columns"
type pa_scrape_variance_fields {
  id: Float
}

"columns and relationships of \"pa_status\""
type pa_status {
  address: String!
  application_validated: timestamptz
  created_at: timestamptz!
  decision: String
  decision_issued_date: timestamptz
  id: String!
  location: geography
  open: Boolean!
  "An array relationship"
  pa_scrape_decideds(
    #distinct select on columns
    distinct_on: [pa_scrape_select_column!],
    #limit the number of rows returned
    limit: Int,
    #skip the first n rows. Use only with order_by
    offset: Int,
    #sort the rows by one or more columns
    order_by: [pa_scrape_order_by!],
    #filter the rows returned
    where: pa_scrape_bool_exp
  ): [pa_scrape!]!
  "An aggregated array relationship"
  pa_scrape_decideds_aggregate(
    #distinct select on columns
    distinct_on: [pa_scrape_select_column!],
    #limit the number of rows returned
    limit: Int,
    #skip the first n rows. Use only with order_by
    offset: Int,
    #sort the rows by one or more columns
    order_by: [pa_scrape_order_by!],
    #filter the rows returned
    where: pa_scrape_bool_exp
  ): pa_scrape_aggregate!
  "An array relationship"
  pa_scrape_validateds(
    #distinct select on columns
    distinct_on: [pa_scrape_validated_depreciated_select_column!],
    #limit the number of rows returned
    limit: Int,
    #skip the first n rows. Use only with order_by
    offset: Int,
    #sort the rows by one or more columns
    order_by: [pa_scrape_validated_depreciated_order_by!],
    #filter the rows returned
    where: pa_scrape_validated_depreciated_bool_exp
  ): [pa_scrape_validated_depreciated!]!
  "An aggregated array relationship"
  pa_scrape_validateds_aggregate(
    #distinct select on columns
    distinct_on: [pa_scrape_validated_depreciated_select_column!],
    #limit the number of rows returned
    limit: Int,
    #skip the first n rows. Use only with order_by
    offset: Int,
    #sort the rows by one or more columns
    order_by: [pa_scrape_validated_depreciated_order_by!],
    #filter the rows returned
    where: pa_scrape_validated_depreciated_bool_exp
  ): pa_scrape_validated_depreciated_aggregate!
  proposal: String!
  status: String
  updated_at: timestamptz!
  url: String!
}

"aggregated selection of \"pa_status\""
type pa_status_aggregate {
  aggregate: pa_status_aggregate_fields
  nodes: [pa_status!]!
}

"aggregate fields of \"pa_status\""
type pa_status_aggregate_fields {
  count(columns: [pa_status_select_column!], distinct: Boolean): Int
  max: pa_status_max_fields
  min: pa_status_min_fields
}

"aggregate max on columns"
type pa_status_max_fields {
  address: String
  application_validated: timestamptz
  created_at: timestamptz
  decision: String
  decision_issued_date: timestamptz
  id: String
  proposal: String
  status: String
  updated_at: timestamptz
  url: String
}

"aggregate min on columns"
type pa_status_min_fields {
  address: String
  application_validated: timestamptz
  created_at: timestamptz
  decision: String
  decision_issued_date: timestamptz
  id: String
  proposal: String
  status: String
  updated_at: timestamptz
  url: String
}

"response of any mutation on the table \"pa_status\""
type pa_status_mutation_response {
  "number of affected rows by the mutation"
  affected_rows: Int!
  "data of the affected rows by the mutation"
  returning: [pa_status!]!
}

"columns and relationships of \"planning_app\""
type planning_app {
  address: String!
  alternative_ref: String
  appeal_decision: String
  appeal_status: String
  created_at: timestamptz!
  decision_date: date
  decision_status: String
  geocode_ok: Boolean
  id: String!
  location: geography
  previousData(
    #JSON select path
    path: String
  ): jsonb
  proposal: String!
  updated_at: timestamptz
  url: String!
  validated_date: date
}

"aggregated selection of \"planning_app\""
type planning_app_aggregate {
  aggregate: planning_app_aggregate_fields
  nodes: [planning_app!]!
}

"aggregate fields of \"planning_app\""
type planning_app_aggregate_fields {
  count(columns: [planning_app_select_column!], distinct: Boolean): Int
  max: planning_app_max_fields
  min: planning_app_min_fields
}

"aggregate max on columns"
type planning_app_max_fields {
  address: String
  alternative_ref: String
  appeal_decision: String
  appeal_status: String
  created_at: timestamptz
  decision_date: date
  decision_status: String
  id: String
  proposal: String
  updated_at: timestamptz
  url: String
  validated_date: date
}

"aggregate min on columns"
type planning_app_min_fields {
  address: String
  alternative_ref: String
  appeal_decision: String
  appeal_status: String
  created_at: timestamptz
  decision_date: date
  decision_status: String
  id: String
  proposal: String
  updated_at: timestamptz
  url: String
  validated_date: date
}

"response of any mutation on the table \"planning_app\""
type planning_app_mutation_response {
  "number of affected rows by the mutation"
  affected_rows: Int!
  "data of the affected rows by the mutation"
  returning: [planning_app!]!
}

"query root"
type query_root {
  "fetch data from the table: \"pa_scrape\""
  pa_scrape(
    #distinct select on columns
    distinct_on: [pa_scrape_select_column!],
    #limit the number of rows returned
    limit: Int,
    #skip the first n rows. Use only with order_by
    offset: Int,
    #sort the rows by one or more columns
    order_by: [pa_scrape_order_by!],
    #filter the rows returned
    where: pa_scrape_bool_exp
  ): [pa_scrape!]!
  "fetch aggregated fields from the table: \"pa_scrape\""
  pa_scrape_aggregate(
    #distinct select on columns
    distinct_on: [pa_scrape_select_column!],
    #limit the number of rows returned
    limit: Int,
    #skip the first n rows. Use only with order_by
    offset: Int,
    #sort the rows by one or more columns
    order_by: [pa_scrape_order_by!],
    #filter the rows returned
    where: pa_scrape_bool_exp
  ): pa_scrape_aggregate!
  "fetch data from the table: \"pa_scrape\" using primary key columns"
  pa_scrape_by_pk(id: Int!): pa_scrape
  "fetch data from the table: \"pa_scrape_validated_depreciated\""
  pa_scrape_validated_depreciated(
    #distinct select on columns
    distinct_on: [pa_scrape_validated_depreciated_select_column!],
    #limit the number of rows returned
    limit: Int,
    #skip the first n rows. Use only with order_by
    offset: Int,
    #sort the rows by one or more columns
    order_by: [pa_scrape_validated_depreciated_order_by!],
    #filter the rows returned
    where: pa_scrape_validated_depreciated_bool_exp
  ): [pa_scrape_validated_depreciated!]!
  "fetch aggregated fields from the table: \"pa_scrape_validated_depreciated\""
  pa_scrape_validated_depreciated_aggregate(
    #distinct select on columns
    distinct_on: [pa_scrape_validated_depreciated_select_column!],
    #limit the number of rows returned
    limit: Int,
    #skip the first n rows. Use only with order_by
    offset: Int,
    #sort the rows by one or more columns
    order_by: [pa_scrape_validated_depreciated_order_by!],
    #filter the rows returned
    where: pa_scrape_validated_depreciated_bool_exp
  ): pa_scrape_validated_depreciated_aggregate!
  "fetch data from the table: \"pa_scrape_validated_depreciated\" using primary key columns"
  pa_scrape_validated_depreciated_by_pk(id: Int!): pa_scrape_validated_depreciated
  "fetch data from the table: \"pa_status\""
  pa_status(
    #distinct select on columns
    distinct_on: [pa_status_select_column!],
    #limit the number of rows returned
    limit: Int,
    #skip the first n rows. Use only with order_by
    offset: Int,
    #sort the rows by one or more columns
    order_by: [pa_status_order_by!],
    #filter the rows returned
    where: pa_status_bool_exp
  ): [pa_status!]!
  "fetch aggregated fields from the table: \"pa_status\""
  pa_status_aggregate(
    #distinct select on columns
    distinct_on: [pa_status_select_column!],
    #limit the number of rows returned
    limit: Int,
    #skip the first n rows. Use only with order_by
    offset: Int,
    #sort the rows by one or more columns
    order_by: [pa_status_order_by!],
    #filter the rows returned
    where: pa_status_bool_exp
  ): pa_status_aggregate!
  "fetch data from the table: \"pa_status\" using primary key columns"
  pa_status_by_pk(id: String!): pa_status
  "fetch data from the table: \"planning_app\""
  planning_app(
    #distinct select on columns
    distinct_on: [planning_app_select_column!],
    #limit the number of rows returned
    limit: Int,
    #skip the first n rows. Use only with order_by
    offset: Int,
    #sort the rows by one or more columns
    order_by: [planning_app_order_by!],
    #filter the rows returned
    where: planning_app_bool_exp
  ): [planning_app!]!
  "fetch aggregated fields from the table: \"planning_app\""
  planning_app_aggregate(
    #distinct select on columns
    distinct_on: [planning_app_select_column!],
    #limit the number of rows returned
    limit: Int,
    #skip the first n rows. Use only with order_by
    offset: Int,
    #sort the rows by one or more columns
    order_by: [planning_app_order_by!],
    #filter the rows returned
    where: planning_app_bool_exp
  ): planning_app_aggregate!
  "fetch data from the table: \"planning_app\" using primary key columns"
  planning_app_by_pk(id: String!): planning_app
  "fetch data from the table: \"scrape_log\""
  scrape_log(
    #distinct select on columns
    distinct_on: [scrape_log_select_column!],
    #limit the number of rows returned
    limit: Int,
    #skip the first n rows. Use only with order_by
    offset: Int,
    #sort the rows by one or more columns
    order_by: [scrape_log_order_by!],
    #filter the rows returned
    where: scrape_log_bool_exp
  ): [scrape_log!]!
  "fetch aggregated fields from the table: \"scrape_log\""
  scrape_log_aggregate(
    #distinct select on columns
    distinct_on: [scrape_log_select_column!],
    #limit the number of rows returned
    limit: Int,
    #skip the first n rows. Use only with order_by
    offset: Int,
    #sort the rows by one or more columns
    order_by: [scrape_log_order_by!],
    #filter the rows returned
    where: scrape_log_bool_exp
  ): scrape_log_aggregate!
  "fetch data from the table: \"scrape_log\" using primary key columns"
  scrape_log_by_pk(id: Int!): scrape_log
  "fetch data from the table: \"users\""
  users(
    #distinct select on columns
    distinct_on: [users_select_column!],
    #limit the number of rows returned
    limit: Int,
    #skip the first n rows. Use only with order_by
    offset: Int,
    #sort the rows by one or more columns
    order_by: [users_order_by!],
    #filter the rows returned
    where: users_bool_exp
  ): [users!]!
  "fetch aggregated fields from the table: \"users\""
  users_aggregate(
    #distinct select on columns
    distinct_on: [users_select_column!],
    #limit the number of rows returned
    limit: Int,
    #skip the first n rows. Use only with order_by
    offset: Int,
    #sort the rows by one or more columns
    order_by: [users_order_by!],
    #filter the rows returned
    where: users_bool_exp
  ): users_aggregate!
  "fetch data from the table: \"users\" using primary key columns"
  users_by_pk(id: String!): users
}

"columns and relationships of \"scrape_log\""
type scrape_log {
  event: String!
  id: Int!
  meta(
    #JSON select path
    path: String
  ): jsonb!
  scraper: String!
  ts: timestamptz
}

"aggregated selection of \"scrape_log\""
type scrape_log_aggregate {
  aggregate: scrape_log_aggregate_fields
  nodes: [scrape_log!]!
}

"aggregate fields of \"scrape_log\""
type scrape_log_aggregate_fields {
  avg: scrape_log_avg_fields
  count(columns: [scrape_log_select_column!], distinct: Boolean): Int
  max: scrape_log_max_fields
  min: scrape_log_min_fields
  stddev: scrape_log_stddev_fields
  stddev_pop: scrape_log_stddev_pop_fields
  stddev_samp: scrape_log_stddev_samp_fields
  sum: scrape_log_sum_fields
  var_pop: scrape_log_var_pop_fields
  var_samp: scrape_log_var_samp_fields
  variance: scrape_log_variance_fields
}

"aggregate avg on columns"
type scrape_log_avg_fields {
  id: Float
}

"aggregate max on columns"
type scrape_log_max_fields {
  event: String
  id: Int
  scraper: String
  ts: timestamptz
}

"aggregate min on columns"
type scrape_log_min_fields {
  event: String
  id: Int
  scraper: String
  ts: timestamptz
}

"response of any mutation on the table \"scrape_log\""
type scrape_log_mutation_response {
  "number of affected rows by the mutation"
  affected_rows: Int!
  "data of the affected rows by the mutation"
  returning: [scrape_log!]!
}

"aggregate stddev on columns"
type scrape_log_stddev_fields {
  id: Float
}

"aggregate stddev_pop on columns"
type scrape_log_stddev_pop_fields {
  id: Float
}

"aggregate stddev_samp on columns"
type scrape_log_stddev_samp_fields {
  id: Float
}

"aggregate sum on columns"
type scrape_log_sum_fields {
  id: Int
}

"aggregate var_pop on columns"
type scrape_log_var_pop_fields {
  id: Float
}

"aggregate var_samp on columns"
type scrape_log_var_samp_fields {
  id: Float
}

"aggregate variance on columns"
type scrape_log_variance_fields {
  id: Float
}

"subscription root"
type subscription_root {
  "fetch data from the table: \"pa_scrape\""
  pa_scrape(
    #distinct select on columns
    distinct_on: [pa_scrape_select_column!],
    #limit the number of rows returned
    limit: Int,
    #skip the first n rows. Use only with order_by
    offset: Int,
    #sort the rows by one or more columns
    order_by: [pa_scrape_order_by!],
    #filter the rows returned
    where: pa_scrape_bool_exp
  ): [pa_scrape!]!
  "fetch aggregated fields from the table: \"pa_scrape\""
  pa_scrape_aggregate(
    #distinct select on columns
    distinct_on: [pa_scrape_select_column!],
    #limit the number of rows returned
    limit: Int,
    #skip the first n rows. Use only with order_by
    offset: Int,
    #sort the rows by one or more columns
    order_by: [pa_scrape_order_by!],
    #filter the rows returned
    where: pa_scrape_bool_exp
  ): pa_scrape_aggregate!
  "fetch data from the table: \"pa_scrape\" using primary key columns"
  pa_scrape_by_pk(id: Int!): pa_scrape
  "fetch data from the table: \"pa_scrape_validated_depreciated\""
  pa_scrape_validated_depreciated(
    #distinct select on columns
    distinct_on: [pa_scrape_validated_depreciated_select_column!],
    #limit the number of rows returned
    limit: Int,
    #skip the first n rows. Use only with order_by
    offset: Int,
    #sort the rows by one or more columns
    order_by: [pa_scrape_validated_depreciated_order_by!],
    #filter the rows returned
    where: pa_scrape_validated_depreciated_bool_exp
  ): [pa_scrape_validated_depreciated!]!
  "fetch aggregated fields from the table: \"pa_scrape_validated_depreciated\""
  pa_scrape_validated_depreciated_aggregate(
    #distinct select on columns
    distinct_on: [pa_scrape_validated_depreciated_select_column!],
    #limit the number of rows returned
    limit: Int,
    #skip the first n rows. Use only with order_by
    offset: Int,
    #sort the rows by one or more columns
    order_by: [pa_scrape_validated_depreciated_order_by!],
    #filter the rows returned
    where: pa_scrape_validated_depreciated_bool_exp
  ): pa_scrape_validated_depreciated_aggregate!
  "fetch data from the table: \"pa_scrape_validated_depreciated\" using primary key columns"
  pa_scrape_validated_depreciated_by_pk(id: Int!): pa_scrape_validated_depreciated
  "fetch data from the table: \"pa_status\""
  pa_status(
    #distinct select on columns
    distinct_on: [pa_status_select_column!],
    #limit the number of rows returned
    limit: Int,
    #skip the first n rows. Use only with order_by
    offset: Int,
    #sort the rows by one or more columns
    order_by: [pa_status_order_by!],
    #filter the rows returned
    where: pa_status_bool_exp
  ): [pa_status!]!
  "fetch aggregated fields from the table: \"pa_status\""
  pa_status_aggregate(
    #distinct select on columns
    distinct_on: [pa_status_select_column!],
    #limit the number of rows returned
    limit: Int,
    #skip the first n rows. Use only with order_by
    offset: Int,
    #sort the rows by one or more columns
    order_by: [pa_status_order_by!],
    #filter the rows returned
    where: pa_status_bool_exp
  ): pa_status_aggregate!
  "fetch data from the table: \"pa_status\" using primary key columns"
  pa_status_by_pk(id: String!): pa_status
  "fetch data from the table: \"planning_app\""
  planning_app(
    #distinct select on columns
    distinct_on: [planning_app_select_column!],
    #limit the number of rows returned
    limit: Int,
    #skip the first n rows. Use only with order_by
    offset: Int,
    #sort the rows by one or more columns
    order_by: [planning_app_order_by!],
    #filter the rows returned
    where: planning_app_bool_exp
  ): [planning_app!]!
  "fetch aggregated fields from the table: \"planning_app\""
  planning_app_aggregate(
    #distinct select on columns
    distinct_on: [planning_app_select_column!],
    #limit the number of rows returned
    limit: Int,
    #skip the first n rows. Use only with order_by
    offset: Int,
    #sort the rows by one or more columns
    order_by: [planning_app_order_by!],
    #filter the rows returned
    where: planning_app_bool_exp
  ): planning_app_aggregate!
  "fetch data from the table: \"planning_app\" using primary key columns"
  planning_app_by_pk(id: String!): planning_app
  "fetch data from the table: \"scrape_log\""
  scrape_log(
    #distinct select on columns
    distinct_on: [scrape_log_select_column!],
    #limit the number of rows returned
    limit: Int,
    #skip the first n rows. Use only with order_by
    offset: Int,
    #sort the rows by one or more columns
    order_by: [scrape_log_order_by!],
    #filter the rows returned
    where: scrape_log_bool_exp
  ): [scrape_log!]!
  "fetch aggregated fields from the table: \"scrape_log\""
  scrape_log_aggregate(
    #distinct select on columns
    distinct_on: [scrape_log_select_column!],
    #limit the number of rows returned
    limit: Int,
    #skip the first n rows. Use only with order_by
    offset: Int,
    #sort the rows by one or more columns
    order_by: [scrape_log_order_by!],
    #filter the rows returned
    where: scrape_log_bool_exp
  ): scrape_log_aggregate!
  "fetch data from the table: \"scrape_log\" using primary key columns"
  scrape_log_by_pk(id: Int!): scrape_log
  "fetch data from the table: \"users\""
  users(
    #distinct select on columns
    distinct_on: [users_select_column!],
    #limit the number of rows returned
    limit: Int,
    #skip the first n rows. Use only with order_by
    offset: Int,
    #sort the rows by one or more columns
    order_by: [users_order_by!],
    #filter the rows returned
    where: users_bool_exp
  ): [users!]!
  "fetch aggregated fields from the table: \"users\""
  users_aggregate(
    #distinct select on columns
    distinct_on: [users_select_column!],
    #limit the number of rows returned
    limit: Int,
    #skip the first n rows. Use only with order_by
    offset: Int,
    #sort the rows by one or more columns
    order_by: [users_order_by!],
    #filter the rows returned
    where: users_bool_exp
  ): users_aggregate!
  "fetch data from the table: \"users\" using primary key columns"
  users_by_pk(id: String!): users
}

"columns and relationships of \"users\""
type users {
  email: String
  id: String!
  location: geography
  name: String
}

"aggregated selection of \"users\""
type users_aggregate {
  aggregate: users_aggregate_fields
  nodes: [users!]!
}

"aggregate fields of \"users\""
type users_aggregate_fields {
  count(columns: [users_select_column!], distinct: Boolean): Int
  max: users_max_fields
  min: users_min_fields
}

"aggregate max on columns"
type users_max_fields {
  email: String
  id: String
  name: String
}

"aggregate min on columns"
type users_min_fields {
  email: String
  id: String
  name: String
}

"response of any mutation on the table \"users\""
type users_mutation_response {
  "number of affected rows by the mutation"
  affected_rows: Int!
  "data of the affected rows by the mutation"
  returning: [users!]!
}

"column ordering options"
enum order_by {
  #in the ascending order, nulls last
  asc
  #in the ascending order, nulls first
  asc_nulls_first
  #in the ascending order, nulls last
  asc_nulls_last
  #in the descending order, nulls first
  desc
  #in the descending order, nulls first
  desc_nulls_first
  #in the descending order, nulls last
  desc_nulls_last
}

"unique or primary key constraints on table \"pa_scrape\""
enum pa_scrape_constraint {
  #unique or primary key constraint
  pa_scrape_decided_pkey
}

"select columns of table \"pa_scrape\""
enum pa_scrape_select_column {
  #column name
  contacts
  #column name
  further_information
  #column name
  id
  #column name
  important_dates
  #column name
  list_type
  #column name
  reference
  #column name
  scraped_at
  #column name
  scraper
  #column name
  summary
  #column name
  url
}

"update columns of table \"pa_scrape\""
enum pa_scrape_update_column {
  #column name
  contacts
  #column name
  further_information
  #column name
  id
  #column name
  important_dates
  #column name
  list_type
  #column name
  reference
  #column name
  scraped_at
  #column name
  scraper
  #column name
  summary
  #column name
  url
}

"unique or primary key constraints on table \"pa_scrape_validated_depreciated\""
enum pa_scrape_validated_depreciated_constraint {
  #unique or primary key constraint
  pa_scrape_validated_pkey
}

"select columns of table \"pa_scrape_validated_depreciated\""
enum pa_scrape_validated_depreciated_select_column {
  #column name
  address
  #column name
  appeal_decision
  #column name
  appeal_status
  #column name
  application_received
  #column name
  application_validated
  #column name
  contacts
  #column name
  decision
  #column name
  decision_issued_date
  #column name
  further_information
  #column name
  id
  #column name
  important_dates
  #column name
  proposal
  #column name
  reference
  #column name
  scraped_at
  #column name
  scraper
  #column name
  status
  #column name
  url
}

"update columns of table \"pa_scrape_validated_depreciated\""
enum pa_scrape_validated_depreciated_update_column {
  #column name
  address
  #column name
  appeal_decision
  #column name
  appeal_status
  #column name
  application_received
  #column name
  application_validated
  #column name
  contacts
  #column name
  decision
  #column name
  decision_issued_date
  #column name
  further_information
  #column name
  id
  #column name
  important_dates
  #column name
  proposal
  #column name
  reference
  #column name
  scraped_at
  #column name
  scraper
  #column name
  status
  #column name
  url
}

"unique or primary key constraints on table \"pa_status\""
enum pa_status_constraint {
  #unique or primary key constraint
  pa_status_pkey
}

"select columns of table \"pa_status\""
enum pa_status_select_column {
  #column name
  address
  #column name
  application_validated
  #column name
  created_at
  #column name
  decision
  #column name
  decision_issued_date
  #column name
  id
  #column name
  location
  #column name
  open
  #column name
  proposal
  #column name
  status
  #column name
  updated_at
  #column name
  url
}

"update columns of table \"pa_status\""
enum pa_status_update_column {
  #column name
  address
  #column name
  application_validated
  #column name
  created_at
  #column name
  decision
  #column name
  decision_issued_date
  #column name
  id
  #column name
  location
  #column name
  open
  #column name
  proposal
  #column name
  status
  #column name
  updated_at
  #column name
  url
}

"unique or primary key constraints on table \"planning_app\""
enum planning_app_constraint {
  #unique or primary key constraint
  planning_app_pkey
  #unique or primary key constraint
  planning_app_ref_key
}

"select columns of table \"planning_app\""
enum planning_app_select_column {
  #column name
  address
  #column name
  alternative_ref
  #column name
  appeal_decision
  #column name
  appeal_status
  #column name
  created_at
  #column name
  decision_date
  #column name
  decision_status
  #column name
  geocode_ok
  #column name
  id
  #column name
  location
  #column name
  previousData
  #column name
  proposal
  #column name
  updated_at
  #column name
  url
  #column name
  validated_date
}

"update columns of table \"planning_app\""
enum planning_app_update_column {
  #column name
  address
  #column name
  alternative_ref
  #column name
  appeal_decision
  #column name
  appeal_status
  #column name
  created_at
  #column name
  decision_date
  #column name
  decision_status
  #column name
  geocode_ok
  #column name
  id
  #column name
  location
  #column name
  previousData
  #column name
  proposal
  #column name
  updated_at
  #column name
  url
  #column name
  validated_date
}

"unique or primary key constraints on table \"scrape_log\""
enum scrape_log_constraint {
  #unique or primary key constraint
  scrape_log_pkey
}

"select columns of table \"scrape_log\""
enum scrape_log_select_column {
  #column name
  event
  #column name
  id
  #column name
  meta
  #column name
  scraper
  #column name
  ts
}

"update columns of table \"scrape_log\""
enum scrape_log_update_column {
  #column name
  event
  #column name
  id
  #column name
  meta
  #column name
  scraper
  #column name
  ts
}

"unique or primary key constraints on table \"users\""
enum users_constraint {
  #unique or primary key constraint
  users_pkey
}

"select columns of table \"users\""
enum users_select_column {
  #column name
  email
  #column name
  id
  #column name
  location
  #column name
  name
}

"update columns of table \"users\""
enum users_update_column {
  #column name
  email
  #column name
  id
  #column name
  location
  #column name
  name
}

"expression to compare columns of type Boolean. All fields are combined with logical 'AND'."
input Boolean_comparison_exp {
  _eq: Boolean
  _gt: Boolean
  _gte: Boolean
  _in: [Boolean!]
  _is_null: Boolean
  _lt: Boolean
  _lte: Boolean
  _neq: Boolean
  _nin: [Boolean!]
}

"expression to compare columns of type Int. All fields are combined with logical 'AND'."
input Int_comparison_exp {
  _eq: Int
  _gt: Int
  _gte: Int
  _in: [Int!]
  _is_null: Boolean
  _lt: Int
  _lte: Int
  _neq: Int
  _nin: [Int!]
}

"expression to compare columns of type String. All fields are combined with logical 'AND'."
input String_comparison_exp {
  _eq: String
  _gt: String
  _gte: String
  _ilike: String
  _in: [String!]
  _is_null: Boolean
  _like: String
  _lt: String
  _lte: String
  _neq: String
  _nilike: String
  _nin: [String!]
  _nlike: String
  _nsimilar: String
  _similar: String
}

"expression to compare columns of type date. All fields are combined with logical 'AND'."
input date_comparison_exp {
  _eq: date
  _gt: date
  _gte: date
  _in: [date!]
  _is_null: Boolean
  _lt: date
  _lte: date
  _neq: date
  _nin: [date!]
}

"Expression to compare the result of casting a column of type geography. Multiple cast targets are combined with logical 'AND'."
input geography_cast_exp {
  geometry: geometry_comparison_exp
}

"expression to compare columns of type geography. All fields are combined with logical 'AND'."
input geography_comparison_exp {
  _cast: geography_cast_exp
  _eq: geography
  _gt: geography
  _gte: geography
  _in: [geography!]
  _is_null: Boolean
  _lt: geography
  _lte: geography
  _neq: geography
  _nin: [geography!]
  "is the column within a distance from a geography value"
  _st_d_within: st_d_within_geography_input
  "does the column spatially intersect the given geography value"
  _st_intersects: geography
}

"Expression to compare the result of casting a column of type geometry. Multiple cast targets are combined with logical 'AND'."
input geometry_cast_exp {
  geography: geography_comparison_exp
}

"expression to compare columns of type geometry. All fields are combined with logical 'AND'."
input geometry_comparison_exp {
  _cast: geometry_cast_exp
  _eq: geometry
  _gt: geometry
  _gte: geometry
  _in: [geometry!]
  _is_null: Boolean
  _lt: geometry
  _lte: geometry
  _neq: geometry
  _nin: [geometry!]
  "does the column contain the given geometry value"
  _st_contains: geometry
  "does the column crosses the given geometry value"
  _st_crosses: geometry
  "is the column within a distance from a geometry value"
  _st_d_within: st_d_within_input
  "is the column equal to given geometry value. Directionality is ignored"
  _st_equals: geometry
  "does the column spatially intersect the given geometry value"
  _st_intersects: geometry
  "does the column 'spatially overlap' (intersect but not completely contain) the given geometry value"
  _st_overlaps: geometry
  "does the column have atleast one point in common with the given geometry value"
  _st_touches: geometry
  "is the column contained in the given geometry value"
  _st_within: geometry
}

"expression to compare columns of type jsonb. All fields are combined with logical 'AND'."
input jsonb_comparison_exp {
  "is the column contained in the given json value"
  _contained_in: jsonb
  "does the column contain the given json value at the top level"
  _contains: jsonb
  _eq: jsonb
  _gt: jsonb
  _gte: jsonb
  "does the string exist as a top-level key in the column"
  _has_key: String
  "do all of these strings exist as top-level keys in the column"
  _has_keys_all: [String!]
  "do any of these strings exist as top-level keys in the column"
  _has_keys_any: [String!]
  _in: [jsonb!]
  _is_null: Boolean
  _lt: jsonb
  _lte: jsonb
  _neq: jsonb
  _nin: [jsonb!]
}

"order by aggregate values of table \"pa_scrape\""
input pa_scrape_aggregate_order_by {
  avg: pa_scrape_avg_order_by
  count: order_by
  max: pa_scrape_max_order_by
  min: pa_scrape_min_order_by
  stddev: pa_scrape_stddev_order_by
  stddev_pop: pa_scrape_stddev_pop_order_by
  stddev_samp: pa_scrape_stddev_samp_order_by
  sum: pa_scrape_sum_order_by
  var_pop: pa_scrape_var_pop_order_by
  var_samp: pa_scrape_var_samp_order_by
  variance: pa_scrape_variance_order_by
}

"append existing jsonb value of filtered columns with new jsonb value"
input pa_scrape_append_input {
  contacts: jsonb
  further_information: jsonb
  important_dates: jsonb
  summary: jsonb
}

"input type for inserting array relation for remote table \"pa_scrape\""
input pa_scrape_arr_rel_insert_input {
  data: [pa_scrape_insert_input!]!
  on_conflict: pa_scrape_on_conflict
}

"order by avg() on columns of table \"pa_scrape\""
input pa_scrape_avg_order_by {
  id: order_by
}

"Boolean expression to filter rows from the table \"pa_scrape\". All fields are combined with a logical 'AND'."
input pa_scrape_bool_exp {
  _and: [pa_scrape_bool_exp]
  _not: pa_scrape_bool_exp
  _or: [pa_scrape_bool_exp]
  contacts: jsonb_comparison_exp
  further_information: jsonb_comparison_exp
  id: Int_comparison_exp
  important_dates: jsonb_comparison_exp
  list_type: String_comparison_exp
  pa_status: pa_status_bool_exp
  reference: String_comparison_exp
  scraped_at: timestamptz_comparison_exp
  scraper: String_comparison_exp
  summary: jsonb_comparison_exp
  url: String_comparison_exp
}

"delete the field or element with specified path (for JSON arrays, negative integers count from the end)"
input pa_scrape_delete_at_path_input {
  contacts: [String]
  further_information: [String]
  important_dates: [String]
  summary: [String]
}

"delete the array element with specified index (negative integers count from the end). throws an error if top level container is not an array"
input pa_scrape_delete_elem_input {
  contacts: Int
  further_information: Int
  important_dates: Int
  summary: Int
}

"delete key/value pair or string element. key/value pairs are matched based on their key value"
input pa_scrape_delete_key_input {
  contacts: String
  further_information: String
  important_dates: String
  summary: String
}

"input type for incrementing integer columne in table \"pa_scrape\""
input pa_scrape_inc_input {
  id: Int
}

"input type for inserting data into table \"pa_scrape\""
input pa_scrape_insert_input {
  contacts: jsonb
  further_information: jsonb
  id: Int
  important_dates: jsonb
  list_type: String
  pa_status: pa_status_obj_rel_insert_input
  reference: String
  scraped_at: timestamptz
  scraper: String
  summary: jsonb
  url: String
}

"order by max() on columns of table \"pa_scrape\""
input pa_scrape_max_order_by {
  id: order_by
  list_type: order_by
  reference: order_by
  scraped_at: order_by
  scraper: order_by
  url: order_by
}

"order by min() on columns of table \"pa_scrape\""
input pa_scrape_min_order_by {
  id: order_by
  list_type: order_by
  reference: order_by
  scraped_at: order_by
  scraper: order_by
  url: order_by
}

"input type for inserting object relation for remote table \"pa_scrape\""
input pa_scrape_obj_rel_insert_input {
  data: pa_scrape_insert_input!
  on_conflict: pa_scrape_on_conflict
}

"on conflict condition type for table \"pa_scrape\""
input pa_scrape_on_conflict {
  constraint: pa_scrape_constraint!
  update_columns: [pa_scrape_update_column!]!
  where: pa_scrape_bool_exp
}

"ordering options when selecting data from \"pa_scrape\""
input pa_scrape_order_by {
  contacts: order_by
  further_information: order_by
  id: order_by
  important_dates: order_by
  list_type: order_by
  pa_status: pa_status_order_by
  reference: order_by
  scraped_at: order_by
  scraper: order_by
  summary: order_by
  url: order_by
}

"prepend existing jsonb value of filtered columns with new jsonb value"
input pa_scrape_prepend_input {
  contacts: jsonb
  further_information: jsonb
  important_dates: jsonb
  summary: jsonb
}

"input type for updating data in table \"pa_scrape\""
input pa_scrape_set_input {
  contacts: jsonb
  further_information: jsonb
  id: Int
  important_dates: jsonb
  list_type: String
  reference: String
  scraped_at: timestamptz
  scraper: String
  summary: jsonb
  url: String
}

"order by stddev() on columns of table \"pa_scrape\""
input pa_scrape_stddev_order_by {
  id: order_by
}

"order by stddev_pop() on columns of table \"pa_scrape\""
input pa_scrape_stddev_pop_order_by {
  id: order_by
}

"order by stddev_samp() on columns of table \"pa_scrape\""
input pa_scrape_stddev_samp_order_by {
  id: order_by
}

"order by sum() on columns of table \"pa_scrape\""
input pa_scrape_sum_order_by {
  id: order_by
}

"order by aggregate values of table \"pa_scrape_validated_depreciated\""
input pa_scrape_validated_depreciated_aggregate_order_by {
  avg: pa_scrape_validated_depreciated_avg_order_by
  count: order_by
  max: pa_scrape_validated_depreciated_max_order_by
  min: pa_scrape_validated_depreciated_min_order_by
  stddev: pa_scrape_validated_depreciated_stddev_order_by
  stddev_pop: pa_scrape_validated_depreciated_stddev_pop_order_by
  stddev_samp: pa_scrape_validated_depreciated_stddev_samp_order_by
  sum: pa_scrape_validated_depreciated_sum_order_by
  var_pop: pa_scrape_validated_depreciated_var_pop_order_by
  var_samp: pa_scrape_validated_depreciated_var_samp_order_by
  variance: pa_scrape_validated_depreciated_variance_order_by
}

"append existing jsonb value of filtered columns with new jsonb value"
input pa_scrape_validated_depreciated_append_input {
  contacts: jsonb
  further_information: jsonb
  important_dates: jsonb
}

"input type for inserting array relation for remote table \"pa_scrape_validated_depreciated\""
input pa_scrape_validated_depreciated_arr_rel_insert_input {
  data: [pa_scrape_validated_depreciated_insert_input!]!
  on_conflict: pa_scrape_validated_depreciated_on_conflict
}

"order by avg() on columns of table \"pa_scrape_validated_depreciated\""
input pa_scrape_validated_depreciated_avg_order_by {
  id: order_by
}

"Boolean expression to filter rows from the table \"pa_scrape_validated_depreciated\". All fields are combined with a logical 'AND'."
input pa_scrape_validated_depreciated_bool_exp {
  _and: [pa_scrape_validated_depreciated_bool_exp]
  _not: pa_scrape_validated_depreciated_bool_exp
  _or: [pa_scrape_validated_depreciated_bool_exp]
  address: String_comparison_exp
  appeal_decision: String_comparison_exp
  appeal_status: String_comparison_exp
  application_received: timestamptz_comparison_exp
  application_validated: timestamptz_comparison_exp
  contacts: jsonb_comparison_exp
  decision: String_comparison_exp
  decision_issued_date: timestamptz_comparison_exp
  further_information: jsonb_comparison_exp
  id: Int_comparison_exp
  important_dates: jsonb_comparison_exp
  pa_status: pa_status_bool_exp
  proposal: String_comparison_exp
  reference: String_comparison_exp
  scraped_at: timestamptz_comparison_exp
  scraper: String_comparison_exp
  status: String_comparison_exp
  url: String_comparison_exp
}

"delete the field or element with specified path (for JSON arrays, negative integers count from the end)"
input pa_scrape_validated_depreciated_delete_at_path_input {
  contacts: [String]
  further_information: [String]
  important_dates: [String]
}

"delete the array element with specified index (negative integers count from the end). throws an error if top level container is not an array"
input pa_scrape_validated_depreciated_delete_elem_input {
  contacts: Int
  further_information: Int
  important_dates: Int
}

"delete key/value pair or string element. key/value pairs are matched based on their key value"
input pa_scrape_validated_depreciated_delete_key_input {
  contacts: String
  further_information: String
  important_dates: String
}

"input type for incrementing integer columne in table \"pa_scrape_validated_depreciated\""
input pa_scrape_validated_depreciated_inc_input {
  id: Int
}

"input type for inserting data into table \"pa_scrape_validated_depreciated\""
input pa_scrape_validated_depreciated_insert_input {
  address: String
  appeal_decision: String
  appeal_status: String
  application_received: timestamptz
  application_validated: timestamptz
  contacts: jsonb
  decision: String
  decision_issued_date: timestamptz
  further_information: jsonb
  id: Int
  important_dates: jsonb
  pa_status: pa_status_obj_rel_insert_input
  proposal: String
  reference: String
  scraped_at: timestamptz
  scraper: String
  status: String
  url: String
}

"order by max() on columns of table \"pa_scrape_validated_depreciated\""
input pa_scrape_validated_depreciated_max_order_by {
  address: order_by
  appeal_decision: order_by
  appeal_status: order_by
  application_received: order_by
  application_validated: order_by
  decision: order_by
  decision_issued_date: order_by
  id: order_by
  proposal: order_by
  reference: order_by
  scraped_at: order_by
  scraper: order_by
  status: order_by
  url: order_by
}

"order by min() on columns of table \"pa_scrape_validated_depreciated\""
input pa_scrape_validated_depreciated_min_order_by {
  address: order_by
  appeal_decision: order_by
  appeal_status: order_by
  application_received: order_by
  application_validated: order_by
  decision: order_by
  decision_issued_date: order_by
  id: order_by
  proposal: order_by
  reference: order_by
  scraped_at: order_by
  scraper: order_by
  status: order_by
  url: order_by
}

"input type for inserting object relation for remote table \"pa_scrape_validated_depreciated\""
input pa_scrape_validated_depreciated_obj_rel_insert_input {
  data: pa_scrape_validated_depreciated_insert_input!
  on_conflict: pa_scrape_validated_depreciated_on_conflict
}

"on conflict condition type for table \"pa_scrape_validated_depreciated\""
input pa_scrape_validated_depreciated_on_conflict {
  constraint: pa_scrape_validated_depreciated_constraint!
  update_columns: [pa_scrape_validated_depreciated_update_column!]!
  where: pa_scrape_validated_depreciated_bool_exp
}

"ordering options when selecting data from \"pa_scrape_validated_depreciated\""
input pa_scrape_validated_depreciated_order_by {
  address: order_by
  appeal_decision: order_by
  appeal_status: order_by
  application_received: order_by
  application_validated: order_by
  contacts: order_by
  decision: order_by
  decision_issued_date: order_by
  further_information: order_by
  id: order_by
  important_dates: order_by
  pa_status: pa_status_order_by
  proposal: order_by
  reference: order_by
  scraped_at: order_by
  scraper: order_by
  status: order_by
  url: order_by
}

"prepend existing jsonb value of filtered columns with new jsonb value"
input pa_scrape_validated_depreciated_prepend_input {
  contacts: jsonb
  further_information: jsonb
  important_dates: jsonb
}

"input type for updating data in table \"pa_scrape_validated_depreciated\""
input pa_scrape_validated_depreciated_set_input {
  address: String
  appeal_decision: String
  appeal_status: String
  application_received: timestamptz
  application_validated: timestamptz
  contacts: jsonb
  decision: String
  decision_issued_date: timestamptz
  further_information: jsonb
  id: Int
  important_dates: jsonb
  proposal: String
  reference: String
  scraped_at: timestamptz
  scraper: String
  status: String
  url: String
}

"order by stddev() on columns of table \"pa_scrape_validated_depreciated\""
input pa_scrape_validated_depreciated_stddev_order_by {
  id: order_by
}

"order by stddev_pop() on columns of table \"pa_scrape_validated_depreciated\""
input pa_scrape_validated_depreciated_stddev_pop_order_by {
  id: order_by
}

"order by stddev_samp() on columns of table \"pa_scrape_validated_depreciated\""
input pa_scrape_validated_depreciated_stddev_samp_order_by {
  id: order_by
}

"order by sum() on columns of table \"pa_scrape_validated_depreciated\""
input pa_scrape_validated_depreciated_sum_order_by {
  id: order_by
}

"order by var_pop() on columns of table \"pa_scrape_validated_depreciated\""
input pa_scrape_validated_depreciated_var_pop_order_by {
  id: order_by
}

"order by var_samp() on columns of table \"pa_scrape_validated_depreciated\""
input pa_scrape_validated_depreciated_var_samp_order_by {
  id: order_by
}

"order by variance() on columns of table \"pa_scrape_validated_depreciated\""
input pa_scrape_validated_depreciated_variance_order_by {
  id: order_by
}

"order by var_pop() on columns of table \"pa_scrape\""
input pa_scrape_var_pop_order_by {
  id: order_by
}

"order by var_samp() on columns of table \"pa_scrape\""
input pa_scrape_var_samp_order_by {
  id: order_by
}

"order by variance() on columns of table \"pa_scrape\""
input pa_scrape_variance_order_by {
  id: order_by
}

"order by aggregate values of table \"pa_status\""
input pa_status_aggregate_order_by {
  count: order_by
  max: pa_status_max_order_by
  min: pa_status_min_order_by
}

"input type for inserting array relation for remote table \"pa_status\""
input pa_status_arr_rel_insert_input {
  data: [pa_status_insert_input!]!
  on_conflict: pa_status_on_conflict
}

"Boolean expression to filter rows from the table \"pa_status\". All fields are combined with a logical 'AND'."
input pa_status_bool_exp {
  _and: [pa_status_bool_exp]
  _not: pa_status_bool_exp
  _or: [pa_status_bool_exp]
  address: String_comparison_exp
  application_validated: timestamptz_comparison_exp
  created_at: timestamptz_comparison_exp
  decision: String_comparison_exp
  decision_issued_date: timestamptz_comparison_exp
  id: String_comparison_exp
  location: geography_comparison_exp
  open: Boolean_comparison_exp
  pa_scrape_decideds: pa_scrape_bool_exp
  pa_scrape_validateds: pa_scrape_validated_depreciated_bool_exp
  proposal: String_comparison_exp
  status: String_comparison_exp
  updated_at: timestamptz_comparison_exp
  url: String_comparison_exp
}

"input type for inserting data into table \"pa_status\""
input pa_status_insert_input {
  address: String
  application_validated: timestamptz
  created_at: timestamptz
  decision: String
  decision_issued_date: timestamptz
  id: String
  location: geography
  open: Boolean
  pa_scrape_decideds: pa_scrape_arr_rel_insert_input
  pa_scrape_validateds: pa_scrape_validated_depreciated_arr_rel_insert_input
  proposal: String
  status: String
  updated_at: timestamptz
  url: String
}

"order by max() on columns of table \"pa_status\""
input pa_status_max_order_by {
  address: order_by
  application_validated: order_by
  created_at: order_by
  decision: order_by
  decision_issued_date: order_by
  id: order_by
  proposal: order_by
  status: order_by
  updated_at: order_by
  url: order_by
}

"order by min() on columns of table \"pa_status\""
input pa_status_min_order_by {
  address: order_by
  application_validated: order_by
  created_at: order_by
  decision: order_by
  decision_issued_date: order_by
  id: order_by
  proposal: order_by
  status: order_by
  updated_at: order_by
  url: order_by
}

"input type for inserting object relation for remote table \"pa_status\""
input pa_status_obj_rel_insert_input {
  data: pa_status_insert_input!
  on_conflict: pa_status_on_conflict
}

"on conflict condition type for table \"pa_status\""
input pa_status_on_conflict {
  constraint: pa_status_constraint!
  update_columns: [pa_status_update_column!]!
  where: pa_status_bool_exp
}

"ordering options when selecting data from \"pa_status\""
input pa_status_order_by {
  address: order_by
  application_validated: order_by
  created_at: order_by
  decision: order_by
  decision_issued_date: order_by
  id: order_by
  location: order_by
  open: order_by
  pa_scrape_decideds_aggregate: pa_scrape_aggregate_order_by
  pa_scrape_validateds_aggregate: pa_scrape_validated_depreciated_aggregate_order_by
  proposal: order_by
  status: order_by
  updated_at: order_by
  url: order_by
}

"input type for updating data in table \"pa_status\""
input pa_status_set_input {
  address: String
  application_validated: timestamptz
  created_at: timestamptz
  decision: String
  decision_issued_date: timestamptz
  id: String
  location: geography
  open: Boolean
  proposal: String
  status: String
  updated_at: timestamptz
  url: String
}

"order by aggregate values of table \"planning_app\""
input planning_app_aggregate_order_by {
  count: order_by
  max: planning_app_max_order_by
  min: planning_app_min_order_by
}

"append existing jsonb value of filtered columns with new jsonb value"
input planning_app_append_input {
  previousData: jsonb
}

"input type for inserting array relation for remote table \"planning_app\""
input planning_app_arr_rel_insert_input {
  data: [planning_app_insert_input!]!
  on_conflict: planning_app_on_conflict
}

"Boolean expression to filter rows from the table \"planning_app\". All fields are combined with a logical 'AND'."
input planning_app_bool_exp {
  _and: [planning_app_bool_exp]
  _not: planning_app_bool_exp
  _or: [planning_app_bool_exp]
  address: String_comparison_exp
  alternative_ref: String_comparison_exp
  appeal_decision: String_comparison_exp
  appeal_status: String_comparison_exp
  created_at: timestamptz_comparison_exp
  decision_date: date_comparison_exp
  decision_status: String_comparison_exp
  geocode_ok: Boolean_comparison_exp
  id: String_comparison_exp
  location: geography_comparison_exp
  previousData: jsonb_comparison_exp
  proposal: String_comparison_exp
  updated_at: timestamptz_comparison_exp
  url: String_comparison_exp
  validated_date: date_comparison_exp
}

"delete the field or element with specified path (for JSON arrays, negative integers count from the end)"
input planning_app_delete_at_path_input {
  previousData: [String]
}

"delete the array element with specified index (negative integers count from the end). throws an error if top level container is not an array"
input planning_app_delete_elem_input {
  previousData: Int
}

"delete key/value pair or string element. key/value pairs are matched based on their key value"
input planning_app_delete_key_input {
  previousData: String
}

"input type for inserting data into table \"planning_app\""
input planning_app_insert_input {
  address: String
  alternative_ref: String
  appeal_decision: String
  appeal_status: String
  created_at: timestamptz
  decision_date: date
  decision_status: String
  geocode_ok: Boolean
  id: String
  location: geography
  previousData: jsonb
  proposal: String
  updated_at: timestamptz
  url: String
  validated_date: date
}

"order by max() on columns of table \"planning_app\""
input planning_app_max_order_by {
  address: order_by
  alternative_ref: order_by
  appeal_decision: order_by
  appeal_status: order_by
  created_at: order_by
  decision_date: order_by
  decision_status: order_by
  id: order_by
  proposal: order_by
  updated_at: order_by
  url: order_by
  validated_date: order_by
}

"order by min() on columns of table \"planning_app\""
input planning_app_min_order_by {
  address: order_by
  alternative_ref: order_by
  appeal_decision: order_by
  appeal_status: order_by
  created_at: order_by
  decision_date: order_by
  decision_status: order_by
  id: order_by
  proposal: order_by
  updated_at: order_by
  url: order_by
  validated_date: order_by
}

"input type for inserting object relation for remote table \"planning_app\""
input planning_app_obj_rel_insert_input {
  data: planning_app_insert_input!
  on_conflict: planning_app_on_conflict
}

"on conflict condition type for table \"planning_app\""
input planning_app_on_conflict {
  constraint: planning_app_constraint!
  update_columns: [planning_app_update_column!]!
  where: planning_app_bool_exp
}

"ordering options when selecting data from \"planning_app\""
input planning_app_order_by {
  address: order_by
  alternative_ref: order_by
  appeal_decision: order_by
  appeal_status: order_by
  created_at: order_by
  decision_date: order_by
  decision_status: order_by
  geocode_ok: order_by
  id: order_by
  location: order_by
  previousData: order_by
  proposal: order_by
  updated_at: order_by
  url: order_by
  validated_date: order_by
}

"prepend existing jsonb value of filtered columns with new jsonb value"
input planning_app_prepend_input {
  previousData: jsonb
}

"input type for updating data in table \"planning_app\""
input planning_app_set_input {
  address: String
  alternative_ref: String
  appeal_decision: String
  appeal_status: String
  created_at: timestamptz
  decision_date: date
  decision_status: String
  geocode_ok: Boolean
  id: String
  location: geography
  previousData: jsonb
  proposal: String
  updated_at: timestamptz
  url: String
  validated_date: date
}

"order by aggregate values of table \"scrape_log\""
input scrape_log_aggregate_order_by {
  avg: scrape_log_avg_order_by
  count: order_by
  max: scrape_log_max_order_by
  min: scrape_log_min_order_by
  stddev: scrape_log_stddev_order_by
  stddev_pop: scrape_log_stddev_pop_order_by
  stddev_samp: scrape_log_stddev_samp_order_by
  sum: scrape_log_sum_order_by
  var_pop: scrape_log_var_pop_order_by
  var_samp: scrape_log_var_samp_order_by
  variance: scrape_log_variance_order_by
}

"append existing jsonb value of filtered columns with new jsonb value"
input scrape_log_append_input {
  meta: jsonb
}

"input type for inserting array relation for remote table \"scrape_log\""
input scrape_log_arr_rel_insert_input {
  data: [scrape_log_insert_input!]!
  on_conflict: scrape_log_on_conflict
}

"order by avg() on columns of table \"scrape_log\""
input scrape_log_avg_order_by {
  id: order_by
}

"Boolean expression to filter rows from the table \"scrape_log\". All fields are combined with a logical 'AND'."
input scrape_log_bool_exp {
  _and: [scrape_log_bool_exp]
  _not: scrape_log_bool_exp
  _or: [scrape_log_bool_exp]
  event: String_comparison_exp
  id: Int_comparison_exp
  meta: jsonb_comparison_exp
  scraper: String_comparison_exp
  ts: timestamptz_comparison_exp
}

"delete the field or element with specified path (for JSON arrays, negative integers count from the end)"
input scrape_log_delete_at_path_input {
  meta: [String]
}

"delete the array element with specified index (negative integers count from the end). throws an error if top level container is not an array"
input scrape_log_delete_elem_input {
  meta: Int
}

"delete key/value pair or string element. key/value pairs are matched based on their key value"
input scrape_log_delete_key_input {
  meta: String
}

"input type for incrementing integer columne in table \"scrape_log\""
input scrape_log_inc_input {
  id: Int
}

"input type for inserting data into table \"scrape_log\""
input scrape_log_insert_input {
  event: String
  id: Int
  meta: jsonb
  scraper: String
  ts: timestamptz
}

"order by max() on columns of table \"scrape_log\""
input scrape_log_max_order_by {
  event: order_by
  id: order_by
  scraper: order_by
  ts: order_by
}

"order by min() on columns of table \"scrape_log\""
input scrape_log_min_order_by {
  event: order_by
  id: order_by
  scraper: order_by
  ts: order_by
}

"input type for inserting object relation for remote table \"scrape_log\""
input scrape_log_obj_rel_insert_input {
  data: scrape_log_insert_input!
  on_conflict: scrape_log_on_conflict
}

"on conflict condition type for table \"scrape_log\""
input scrape_log_on_conflict {
  constraint: scrape_log_constraint!
  update_columns: [scrape_log_update_column!]!
  where: scrape_log_bool_exp
}

"ordering options when selecting data from \"scrape_log\""
input scrape_log_order_by {
  event: order_by
  id: order_by
  meta: order_by
  scraper: order_by
  ts: order_by
}

"prepend existing jsonb value of filtered columns with new jsonb value"
input scrape_log_prepend_input {
  meta: jsonb
}

"input type for updating data in table \"scrape_log\""
input scrape_log_set_input {
  event: String
  id: Int
  meta: jsonb
  scraper: String
  ts: timestamptz
}

"order by stddev() on columns of table \"scrape_log\""
input scrape_log_stddev_order_by {
  id: order_by
}

"order by stddev_pop() on columns of table \"scrape_log\""
input scrape_log_stddev_pop_order_by {
  id: order_by
}

"order by stddev_samp() on columns of table \"scrape_log\""
input scrape_log_stddev_samp_order_by {
  id: order_by
}

"order by sum() on columns of table \"scrape_log\""
input scrape_log_sum_order_by {
  id: order_by
}

"order by var_pop() on columns of table \"scrape_log\""
input scrape_log_var_pop_order_by {
  id: order_by
}

"order by var_samp() on columns of table \"scrape_log\""
input scrape_log_var_samp_order_by {
  id: order_by
}

"order by variance() on columns of table \"scrape_log\""
input scrape_log_variance_order_by {
  id: order_by
}

input st_d_within_geography_input {
  distance: Float!
  from: geography!
  use_spheroid: Boolean = true
}

input st_d_within_input {
  distance: Float!
  from: geometry!
}

"expression to compare columns of type timestamptz. All fields are combined with logical 'AND'."
input timestamptz_comparison_exp {
  _eq: timestamptz
  _gt: timestamptz
  _gte: timestamptz
  _in: [timestamptz!]
  _is_null: Boolean
  _lt: timestamptz
  _lte: timestamptz
  _neq: timestamptz
  _nin: [timestamptz!]
}

"order by aggregate values of table \"users\""
input users_aggregate_order_by {
  count: order_by
  max: users_max_order_by
  min: users_min_order_by
}

"input type for inserting array relation for remote table \"users\""
input users_arr_rel_insert_input {
  data: [users_insert_input!]!
  on_conflict: users_on_conflict
}

"Boolean expression to filter rows from the table \"users\". All fields are combined with a logical 'AND'."
input users_bool_exp {
  _and: [users_bool_exp]
  _not: users_bool_exp
  _or: [users_bool_exp]
  email: String_comparison_exp
  id: String_comparison_exp
  location: geography_comparison_exp
  name: String_comparison_exp
}

"input type for inserting data into table \"users\""
input users_insert_input {
  email: String
  id: String
  location: geography
  name: String
}

"order by max() on columns of table \"users\""
input users_max_order_by {
  email: order_by
  id: order_by
  name: order_by
}

"order by min() on columns of table \"users\""
input users_min_order_by {
  email: order_by
  id: order_by
  name: order_by
}

"input type for inserting object relation for remote table \"users\""
input users_obj_rel_insert_input {
  data: users_insert_input!
  on_conflict: users_on_conflict
}

"on conflict condition type for table \"users\""
input users_on_conflict {
  constraint: users_constraint!
  update_columns: [users_update_column!]!
  where: users_bool_exp
}

"ordering options when selecting data from \"users\""
input users_order_by {
  email: order_by
  id: order_by
  location: order_by
  name: order_by
}

"input type for updating data in table \"users\""
input users_set_input {
  email: String
  id: String
  location: geography
  name: String
}


scalar date

scalar geography

scalar geometry

scalar jsonb

scalar timestamptz
